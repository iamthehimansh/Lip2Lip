{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prepare App\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install TTS\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "# !pip install jiwer\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "# @title Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prepare for Low Quality\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/justinjohn0306/Wav2Lip\n",
        "!cd Wav2Lip && pip install -r requirements_colab.txt\n",
        "\n",
        "%cd /content/Wav2Lip\n",
        "\n",
        "# !wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"face_detection/detection/sfd/s3fd.pth\"\n",
        "# !wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip.pth' -O 'checkpoints/wav2lip.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip_gan.pth' -O 'checkpoints/wav2lip_gan.pth'\n",
        "# !wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/resnet50.pth' -O 'checkpoints/resnet50.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/mobilenet.pth' -O 'checkpoints/mobilenet.pth'\n",
        "\n",
        "!pip install batch-face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prepare for High Quality\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/vinthony/video-retalking.git &> /dev/null\n",
        "\n",
        "!sudo apt-get install -y libblas-dev liblapack-dev libx11-dev libopenblas-dev\n",
        "\n",
        "!git clone https://github.com/davisking/dlib.git\n",
        "\n",
        "!pip install basicsr==1.4.2 face-alignment==1.3.4 kornia==0.5.1 ninja==1.10.2.3 einops==0.4.1 facexlib==0.2.5 librosa==0.9.2 build\n",
        "\n",
        "!cd dlib && python setup.py install\n",
        "\n",
        "%cd /content/video-retalking\n",
        "\n",
        "!mkdir ./checkpoints\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/30_net_gen.pth -O ./checkpoints/30_net_gen.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/BFM.zip -O ./checkpoints/BFM.zip\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/DNet.pt -O ./checkpoints/DNet.pt\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ENet.pth -O ./checkpoints/ENet.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/expression.mat -O ./checkpoints/expression.mat\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth -O ./checkpoints/face3d_pretrain_epoch_20.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth -O ./checkpoints/GFPGANv1.3.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth -O ./checkpoints/GPEN-BFR-512.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/LNet.pth -O ./checkpoints/LNet.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth -O ./checkpoints/ParseNet-latest.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth -O ./checkpoints/RetinaFace-R50.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat -O ./checkpoints/shape_predictor_68_face_landmarks.dat\n",
        "!unzip -d ./checkpoints/BFM ./checkpoints/BFM.zip\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/vinthony/video-retalking.git &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run App \n",
        "from flask import Flask, request, jsonify,send_from_directory,render_template\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "import subprocess\n",
        "import whisper\n",
        "from googletrans import Translator\n",
        "from TTS.api import TTS\n",
        "import torch\n",
        "app = Flask(__name__)\n",
        "import threading\n",
        "PORT=5005\n",
        "import requests\n",
        "response = requests.get('https://raw.githubusercontent.com/iamthehimansh/Lip2Lip/main/templates/index.html')\n",
        "os.makedirs('templates', exist_ok=True)\n",
        "with open('templates/index.html', 'w') as f:\n",
        "    f.write(response.text)\n",
        "def func():\n",
        "    !ssh -o \"StrictHostKeyChecking=no\" -R 80:localhost:{PORT} serveo.net\n",
        "threading.Thread(target=func,daemon=True).start()\n",
        "# Set the upload folder for video and audio files\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
        "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "def resize_video(filename):\n",
        "    output_filename = f\"resized_{filename}\"\n",
        "    cmd = f\"ffmpeg -i {filename} -vf 'scale=-1:720' {output_filename}\"\n",
        "    subprocess.run(cmd, shell=True)\n",
        "    print(f'Resized video saved as {output_filename}')\n",
        "    return output_filename\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "@app.route('/process', methods=['POST'])\n",
        "def generate_video():\n",
        "    # Check if the request contains video, audio, and text files\n",
        "    if 'video' not in request.files or 'audio' not in request.files or 'text' not in request.form:\n",
        "        return jsonify({'error': 'Video, audio, and text files are required.'}), 400\n",
        "\n",
        "    video_file = request.files['video']\n",
        "    audio_file = request.files['audio']\n",
        "    text = request.form['text']\n",
        "    quality = request.form['quality']\n",
        "\n",
        "    # Save the video and audio files to the upload folder\n",
        "    video_filename = secure_filename(video_file.filename)\n",
        "    audio_filename = secure_filename(audio_file.filename)\n",
        "    video_path = os.path.join(app.config['UPLOAD_FOLDER'], video_filename)\n",
        "    audio_path = os.path.join(app.config['UPLOAD_FOLDER'], audio_filename)\n",
        "    video_file.save(video_path)\n",
        "    audio_file.save(audio_path)\n",
        "\n",
        "    # Generate the video based on the provided files and text\n",
        "    # Replace this code with your video generation logic\n",
        "    generated_video_path,code = generate_video_function(video_path, audio_path, text,quality)\n",
        "\n",
        "    # Check if the video generation was successful\n",
        "    if generated_video_path is None:\n",
        "        return jsonify({'error': 'Failed to generate the video.'}), 500\n",
        "\n",
        "    # Return the generated video file\n",
        "    return generated_video_path,code\n",
        "\n",
        "def generate_video_function(video_path, audio_path_, text_,quality=\"low\"):\n",
        "    # resized_video_path = resize_video(video_path)\n",
        "    resized_video_path=video_path\n",
        "    # @title Audio extraction (24 bit) and whisper conversion\n",
        "    \n",
        "\n",
        "    # Ensure video_path variable exists and is not None\n",
        "    if resized_video_path and resized_video_path is not None:\n",
        "        print(\"here\")\n",
        "        ffmpeg_command = f\"ffmpeg -i '{resized_video_path}' -acodec pcm_s24le -ar 48000 -q:a 0 -map a -y 'output_audio_1.wav'\"\n",
        "        subprocess.run(ffmpeg_command, shell=True)\n",
        "    else:\n",
        "        print(\"No video uploaded. Please upload a video first.\")\n",
        "\n",
        "\n",
        "    # model = whisper.load_model(\"base\")\n",
        "    # result = model.transcribe(\"output_audio_1.wav\")\n",
        "\n",
        "    # whisper_text = result[\"text\"]\n",
        "    # whisper_language = result['language']\n",
        "\n",
        "    # print(\"Whisper text:\", whisper_text)\n",
        "    # #@title Translation with Google Translate\n",
        "    # # Mapping between full names and ISO 639-1 codes\n",
        "    # language_mapping = {\n",
        "    #     'English': 'en',\n",
        "    #     'Spanish': 'es',\n",
        "    #     'French': 'fr',\n",
        "    #     'German': 'de',\n",
        "    #     'Italian': 'it',\n",
        "    #     'Portuguese': 'pt',\n",
        "    #     'Polish': 'pl',\n",
        "    #     'Turkish': 'tr',\n",
        "    #     'Russian': 'ru',\n",
        "    #     'Dutch': 'nl',\n",
        "    #     'Czech': 'cs',\n",
        "    #     'Arabic': 'ar',\n",
        "    #     'Chinese (Simplified)': 'zh-cn',\n",
        "    #     'Hindi': 'hi',\n",
        "    # }\n",
        "\n",
        "    # # Dropdown with full names\n",
        "    # target_language = \"Hindi\" #@param [\"English\", \"Spanish\", \"French\", \"German\", \"Italian\", \"Portuguese\", \"Polish\", \"Turkish\", \"Russian\", \"Dutch\", \"Czech\", \"Arabic\", \"Chinese (Simplified)\", \"Hindi\"]\n",
        "\n",
        "    # # Convert full name to ISO 639-1 code\n",
        "    # target_language_code = language_mapping[target_language]\n",
        "\n",
        "    # Assume whisper_text and whisper_language are defined from previous code\n",
        "\n",
        "    # Initialize the translator\n",
        "    # translator = Translator()\n",
        "\n",
        "    # # Translate the text\n",
        "    # translated_text = translator.translate(whisper_text, src=whisper_language, dest=target_language_code).text\n",
        "\n",
        "    # # Output the translated text\n",
        "    # print(\"Translated text:\", translated_text)\n",
        "    # # @title split texts into 250 character chunks (Hindi)\n",
        "    # text_chunks = translated_text.split(sep = \"।\")\n",
        "    # final_chunks = [\"\"]\n",
        "    # for chunk in text_chunks:\n",
        "    # if not final_chunks[-1] or len(final_chunks[-1])+len(chunk)<250:\n",
        "    #     chunk += \"।\"\n",
        "    #     final_chunks[-1]+=chunk.strip()\n",
        "    # else:\n",
        "    #     final_chunks.append(chunk+\"।\".strip())\n",
        "    # @title Voice synthesis\n",
        "    \n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # Initialize TTS\n",
        "    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
        "\n",
        "    # Generate audio file\n",
        "    # tts.tts_to_file(speech_fr,\n",
        "    #     speaker_wav='output_audio_1.wav',\n",
        "    #     file_path=\"output_synth_fr.wav\",\n",
        "    #     language=\"fr\"\n",
        "    # )\n",
        "    # # @title Voice synthesis for text chunks\n",
        "    # def audio_synthesis(text, file_name):\n",
        "    #     tts.tts_to_file(\n",
        "    #         text,\n",
        "    #         speaker_wav='output_audio_1.wav',\n",
        "    #         file_path=file_name,\n",
        "    #         language=\"hi\"\n",
        "    #     )\n",
        "    #     return file_name\n",
        "    # file_names = []\n",
        "    # for i in range(len(final_chunks)):\n",
        "    #     file_name = audio_synthesis(final_chunks[i], f\"output_synth_audio_{i}.wav\")\n",
        "    #     file_names.append(file_name)\n",
        "\n",
        "    # create a text file and add paths to files to be merged()\n",
        "    # %rm -rf my_files.txt\n",
        "    # %touch my_files.txt\n",
        "\n",
        "    # #concat audios\n",
        "    # cmd = \"ffmpeg -f concat -safe 0 -i my_files.txt -c copy output_synth_audio_final.wav\"\n",
        "    # subprocess.run(cmd, shell=True)\n",
        "    mutual_fund_speech=text_\n",
        "    %rm -rf /content/output_synth_audio_final.wav\n",
        "    tts.tts_to_file(mutual_fund_speech,\n",
        "        speaker_wav=audio_path_,\n",
        "        file_path=\"output_synth_audio_final.wav\",\n",
        "        language=\"en\"\n",
        "        )\n",
        "    try:\n",
        "        del tts\n",
        "    except NameError:\n",
        "        print(\"Voice model already deleted\")\n",
        "\n",
        "    try:\n",
        "        del model\n",
        "    except NameError:\n",
        "        print(\"Whisper model already deleted\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if quality == \"low\":\n",
        "        %cd /content/Wav2Lip\n",
        "\n",
        "        #This is the detection box padding, if you see it doesnt sit quite right, just adjust the values a bit. Usually the bottom one is the biggest issue\n",
        "        pad_top =  0\n",
        "        pad_bottom =  15\n",
        "        pad_left =  0\n",
        "        pad_right =  0\n",
        "        rescaleFactor =  1\n",
        "\n",
        "        video_path_fix = f\"'../{resized_video_path}'\"\n",
        "\n",
        "        !python inference.py --checkpoint_path 'checkpoints/wav2lip_gan.pth' --face ${video_path_fix} --audio \"/content/output_synth_audio_final.wav\" --pads ${pad_top} ${pad_bottom} ${pad_left} ${pad_right} --resize_factor ${rescaleFactor} --nosmooth --outfile '/content/output_high_qual_hi.mp4'\n",
        "    else:\n",
        "        %cd /content/video-retalking\n",
        "\n",
        "        video_path_fix = f\"'../{resized_video_path}'\"\n",
        "        %rm -rf /content/output_high_qual_hi.mp4\n",
        "        !python inference.py \\\n",
        "        --face ${video_path_fix} \\\n",
        "        --audio {\"/content/output_synth_audio_final.wav\"} \\\n",
        "        --outfile {'/content/output_high_qual_hi.mp4'}\n",
        "    \n",
        "        return \"error\",400\n",
        "    return send_from_directory('/content', 'output_high_qual_hi.mp4'),200\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host=\"0.0.0.0\",port=PORT)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LaN9kFg9Oa-r"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
